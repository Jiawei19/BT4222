{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae8188f",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout, SimpleRNN\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bf77c",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c67927-8a43-48df-a0c7-03bad41b6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('news_final_dataset.pickle', 'rb') as file:\n",
    "    news = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef901a-9c5f-4ccd-b091-d03b3c2fe641",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = news['X_train'], news['X_test'], news['y_train'], news['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741abbf7-93fe-4e27-b2a7-61cb9edeaba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for needed columns\n",
    "X_train = X_train[['title_vect','all_text_vect']]\n",
    "X_test = X_test[['title_vect','all_text_vect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4071a-c4f9-4518-b763-af7de95d623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further split train set into smaller train set + validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1, test_size=0.25) # 0.25 x 0.8 = 0.2, #20% valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb248379-fce4-49d0-bdc6-f3237c42d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71127efb",
   "metadata": {},
   "source": [
    "# Hypothesis 2: Testing Neural Network models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71270f90",
   "metadata": {},
   "source": [
    "## Define Tokenizing, Padding, Plotting functions & Earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(train_data,val_data,test_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "    train_data = tokenizer.texts_to_sequences(train_data)\n",
    "    val_data = tokenizer.texts_to_sequences(val_data)\n",
    "    test_data = tokenizer.texts_to_sequences(test_data)\n",
    "    vocab = tokenizer.word_index\n",
    "    return train_data,val_data,test_data,vocab\n",
    "\n",
    "def pad(train_data,val_data,test_data,max_len):\n",
    "    train_data = pad_sequences(train_data, padding='post', maxlen=max_len)\n",
    "    val_data = pad_sequences(val_data, padding='post', maxlen=max_len)\n",
    "    test_data = pad_sequences(test_data, padding='post', maxlen=max_len)\n",
    "    return train_data,val_data,test_data\n",
    "\n",
    "def plotHistory(history):\n",
    "    history_dict = history.history\n",
    "\n",
    "    acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    epochs = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159e64b",
   "metadata": {},
   "source": [
    "## 1. Training on titles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525afd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val1, X_test1, vocab1 = tokenize(X_train['title'], X_val['title'], X_test['title'])\n",
    "max_vocab1 = len(vocab1) + 1 # Adding 1 because of reserved 0 index\n",
    "# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n",
    "print(\"Vocab Size for Titles: {}\".format(max_vocab1))\n",
    "\n",
    "# get length of longest title and pad all shorter titles to match length\n",
    "max_len1 = max([len(x) for x in X_train1] + [len(x) for x in X_test1])\n",
    "X_train1, X_val1, X_test1 = pad(X_train1, X_val1, X_test1, max_len1)\n",
    "print(\"Max Title Length: {}\".format(max_len1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a73057",
   "metadata": {},
   "source": [
    "### Model 1: Training a simple RNN model on titles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ea078",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_title_model = Sequential([\n",
    "    Embedding(max_vocab1, 100, input_length=max_len1),\n",
    "    Bidirectional(SimpleRNN(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_title_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnn_title_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd78af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rnn_title_history = rnn_title_model.fit(X_train1, y_train, epochs=10, \n",
    "                    validation_data=(X_val1, y_val), batch_size=30, \n",
    "                    shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(rnn_title_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b04c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_title_model.evaluate(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_title_model.evaluate(X_val1, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_title_model.evaluate(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d9a9b",
   "metadata": {},
   "source": [
    "### Model 2: Training an LSTM model on titles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a81a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_title_model = Sequential([\n",
    "    Embedding(max_vocab1, 100, input_length=max_len1),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_title_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm_title_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea35f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lstm_title_history = lstm_title_model.fit(X_train1, y_train, epochs=10, \n",
    "                    validation_data=(X_val1, y_val), batch_size=30, \n",
    "                    shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad60e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(lstm_title_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_title_model.evaluate(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_title_model.evaluate(X_val1, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_title_model.evaluate(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4ce12",
   "metadata": {},
   "source": [
    "## 2. Training on all text (title + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792496dd",
   "metadata": {},
   "source": [
    "### Training the word2vec model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train['all_text']\n",
    "X_val2 = X_val['all_text']\n",
    "X_test2 = X_test['all_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # all tokens generated in the first article\n",
    "print(X_train2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec on the 'text' corpus to form the embedding layer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "w2v = Word2Vec(\n",
    "    sentences = X_train2,\n",
    "    vector_size = EMBEDDING_DIM,\n",
    "    window = 5,\n",
    "    min_count = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99aa241",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(w2v.wv)\n",
    "print('There are a total of %d words in the vocabulary of our trained word2vec model.' % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304daac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv[\"donald\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08aba56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preparing the neural network model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_val2, X_test2, vocab2 = tokenize(X_train2, X_val2, X_test2)\n",
    "max_vocab2 = len(vocab2) + 1\n",
    "print(\"Vocab Size for All Text: {}\".format(max_vocab2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all token indexes generated from the first article\n",
    "print(X_train2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20300d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lens = np.array([len(index) for index in X_train2])\n",
    "plt.hist(article_lens, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_1000 = article_lens[article_lens < 1000]\n",
    "print('%d out of %d articles have less than 1000 words' % (len(len_1000),len(article_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7f516",
   "metadata": {},
   "source": [
    "<li>Since the inputs to the neural network have to be of the same size, we have to pad each article in the dataset.</li>\n",
    "<li>Since the majority of articles have less than 1000 words, we pad shorter news articles and truncate longer articles.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30971e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len2 = 1000\n",
    "X_train2, X_val2, X_test2 = pad(X_train2, X_val2, X_test2, max_len2)\n",
    "print(\"Max All Text Length: {}\".format(max_len2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a weight matrix to retain weights learned by word2vec in the embedding layer later on\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # create weight matrix with number of rows = vocab_size and number of columns = number of embedding dimensions\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # for each word in the vocab, store its vector created by the word2vec model\n",
    "    for word, row_index in vocab.items():\n",
    "        weight_matrix[row_index] = model.wv[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = get_weight_matrix(w2v, vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8828b9e",
   "metadata": {},
   "source": [
    "### Model 3: Training a simple RNN model on all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9afa498",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_alltext_model = Sequential([\n",
    "    Embedding(max_vocab2, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=max_len2, trainable=False),\n",
    "    Bidirectional(SimpleRNN(units=128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_alltext_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnn_alltext_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rnn_alltext_history = rnn_alltext_model.fit(X_train2, y_train, epochs=10, \n",
    "                                validation_data=(X_val2, y_val), batch_size=30, \n",
    "                                shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(rnn_alltext_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b86caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_alltext_model.evaluate(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf60d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_alltext_model.evaluate(X_val2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_alltext_model.evaluate(X_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e5f0c",
   "metadata": {},
   "source": [
    "### Model 4: Training an LSTM model on all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_alltext_model = Sequential([\n",
    "    Embedding(max_vocab2, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=max_len2, trainable=False),\n",
    "    Bidirectional(LSTM(units=128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_alltext_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm_alltext_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lstm_alltext_history = lstm_alltext_model.fit(X_train2, y_train, epochs=10, \n",
    "                                validation_data=(X_val2, y_val), batch_size=30, \n",
    "                                shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(lstm_alltext_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_alltext_model.evaluate(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619deb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_alltext_model.evaluate(X_val2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e94f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_alltext_model.evaluate(X_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059187a",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for LSTM model for all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0503dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameterTuning(hp): #https://keras.io/api/keras_tuner/hyperparameters/\n",
    "    model = Sequential([\n",
    "        Embedding(max_vocab2, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=max_len2, trainable=False),\n",
    "        Bidirectional(LSTM(units=hp.Int('units', min_value=64, max_value=192, step=64))), #https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "        Dropout(rate=hp.Float('rate', min_value=0.3, max_value=0.7, step=0.2)), #https://keras.io/api/layers/regularization_layers/dropout/\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/swlh/hyperparameter-tuning-in-keras-tensorflow-2-with-keras-tuner-randomsearch-hyperband-3e212647778f\n",
    "tuner = keras_tuner.BayesianOptimization(hyperparameterTuning,\n",
    "                              objective='val_loss',\n",
    "                             directory='lstm_tuner2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13217b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "tuner.search(X_train2, y_train, epochs=10, \n",
    "            validation_data=(X_val2, y_val), batch_size=30, \n",
    "            shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model.evaluate(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model.evaluate(X_val2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model.evaluate(X_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5becb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0a6d3a1c9702cbdea61a84eb923333e247e4db67d8a1f6fe21a7310eabf978f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
