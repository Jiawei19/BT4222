{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8542fcc8",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131754ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Kaggle username and key \n",
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = # username\n",
    "os.environ['KAGGLE_KEY'] = # key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457765ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
    "!unzip fake-and-real-news-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d923ce",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "import gensim\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42d007-999d-42ff-9110-303d741f51c5",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f673ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = pd.read_csv(\"Fake.csv\")\n",
    "real_news = pd.read_csv(\"True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73471851",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb49dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d55d88-94fb-40c6-8b34-42031d3f091d",
   "metadata": {},
   "source": [
    "<li>Creating the target variable 'fake', with a value of 1 if an article is fake and 0 otherwise.\n",
    "<li>Joining both dataframes to form the complete dataframe.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61354b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news['fake'] = 1\n",
    "real_news['fake'] = 0\n",
    "news = pd.concat([fake_news, real_news])\n",
    "news.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f198e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0441395-06d8-459c-bc1e-c0cf23e059b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = news['fake'].value_counts()\n",
    "fake = counts[1]\n",
    "real = counts[0]\n",
    "print('The dataset consists of %d fake news articles and %d real news articles' % (fake,real))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328064d-cc7f-4c14-873d-235dd40a5c0d",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa2eed-0152-4c56-8eae-d1a88f5543f5",
   "metadata": {},
   "source": [
    "### 2.1 Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing data\n",
    "news.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dd744-3861-4d83-8fb9-a910b3d8802f",
   "metadata": {},
   "source": [
    "### 2.2 Duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c53e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "news['all_text'] = news['title'] + ' '+ news['text'] \n",
    "num_dup = len(news) - len(news['all_text'].unique())\n",
    "print('There are a total of %d duplicates in the dataset' % num_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd44fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.drop_duplicates(subset=['all_text'], inplace = True)\n",
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_counts = news['fake'].value_counts()\n",
    "updated_fake = updated_counts[1]\n",
    "updated_real = updated_counts[0]\n",
    "print('After cleaning, the dataset consists of %d fake news articles and %d real news articles' % (updated_fake,updated_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x='fake', data=news, palette=['g','r'])\n",
    "ax.set_title('Fake Breakdown')\n",
    "for i in ax.patches:\n",
    "    ax.annotate(f'\\n{i.get_height()}', (i.get_x() + 0.3, i.get_height()), ha='center', va='top', color='white', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfdd08",
   "metadata": {},
   "source": [
    "After removing duplicates, we now have 21197 real news and 17908 fake news articles to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb17107-f5df-42b1-9a4d-ee5b5f869476",
   "metadata": {},
   "source": [
    "### 2.3 Removing publisher information for real news\n",
    "<li>With a quick scan of the real news articles in our dataset, we realise that most texts start off with publisher information, specifically Reuters <b>(eg. 'WASHINGTON (Reuters) -')</b>.</li>\n",
    "<li>We want to be able to generalise our predictions models to articles from any publisher, hence we drop publisher information from our dataset.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd36fb-2466-4c4a-ac17-dc6be1475e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_publisher_info = news['text'].str.contains('^.+ \\(Reuters\\) - ', regex=True).sum()\n",
    "print('A total of %d out of %d real news articles contain publisher information' % (with_publisher_info,updated_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271c226-55c0-4820-864a-a2ff48b7ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['text'] = news['text'].map(lambda x: re.sub(r'^.+ \\(Reuters\\) - ','',x))\n",
    "news[news['fake']== 0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1d3dc-95c3-4875-b646-683fe0c18708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update 'all_text' column\n",
    "news['all_text'] = news['title'] + ' ' + news['text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ad8f1",
   "metadata": {},
   "source": [
    "### 2.4 Articles with no text body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['length'] = news['text'].map(lambda x: len(x.split()))\n",
    "news[news['length']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a485f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 446 fake articles with no text body, 1 real article with no text body\n",
    "news[news['length']==0]['fake'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd332e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bf391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.countplot(x='subject', data=news, hue='fake', palette=['g','r'])\n",
    "ax.set_title('Subject Distribution')\n",
    "ax.set(ylim=(0, 14000))\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'\\n{p.get_height().astype(int)}', (p.get_x()+0.2, p.get_height() + 1000), ha='center', va='top', color='black', size=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting subject distribution by target\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "g = sns.catplot(x=\"subject\", col=\"fake\", data=news, kind=\"count\")\n",
    "\n",
    "#Rotating the xlabels\n",
    "g.set_xticklabels(rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa20ae-bc8c-4d4c-846a-daff05ae0e28",
   "metadata": {},
   "source": [
    "It appears that all real news fall under either of the subjects 'politicsNews' or 'worldnews', while a significant proportion of fake news are categorised as 'News' or 'politics'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall word length distribution\n",
    "news['length'] = news['text'].map(lambda x: len(x.split()))\n",
    "sns.histplot(x='length', data=news, bins = 50, hue='fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake data distribution of word length\n",
    "sns.histplot(x='length', data=news.loc[news['fake'] == 1], bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95154a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='length', data=news.loc[news['fake'] == 0], bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee48d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the 2 plots above \n",
    "\n",
    "g = sns.FacetGrid(news, col=\"fake\")\n",
    "g.map(sns.histplot, \"length\", binwidth=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "#Word Cloud\n",
    "stopwords = set(STOPWORDS)\n",
    "def give_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=0\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3669293",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_wordcloud(news,'All News')\n",
    "give_wordcloud(news[news['fake'] == 1], 'Fake News')\n",
    "give_wordcloud(news[news['fake'] == 0], 'Real News')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42778c",
   "metadata": {},
   "source": [
    "Fake news shows emotive and loaded language such as 'Drunk' and 'Obsessed', and biased persons such as Donald Trump are often mentioned.\n",
    "Real news shows distinguished establishments such as 'NATO'and 'Reuters', and does not contain any emotive language. Instead, the words lean more towards factual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Getting sentiment values from all_text\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# news['sentiment_score'] = [analyzer.polarity_scores(x)['compound'] for x in news['all_text']]\n",
    "# news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db9bb8-035c-4829-b3b0-21589aa6c10e",
   "metadata": {},
   "source": [
    "# 4. Testing our hypotheses:\n",
    "<li></li>\n",
    "<li></li>\n",
    "<li></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb05939-0323-4b13-87c0-6b9d3397f911",
   "metadata": {},
   "source": [
    "# 5. Neural Network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce9ddc-990a-4f62-9b7f-e054641d8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = news[['title','text','all_text','fake']]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fbd5f-9e60-4159-a147-d53969900f8e",
   "metadata": {},
   "source": [
    "### Text cleaning for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(data):\n",
    "    i = data.lower()\n",
    "    # get rid of urls\n",
    "    i = re.sub('https?://\\S+|www\\.\\S+', '', i)\n",
    "    #get rid of non words and extra spaces\n",
    "    i = re.sub('\\\\W', ' ', i)\n",
    "    i = re.sub('\\n', '', i)\n",
    "    i = re.sub(' +', ' ', i)\n",
    "    i = re.sub('^ ', '', i)\n",
    "    i = re.sub(' $', '', i)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c2bbc-ffc3-4b4d-b9a6-09f3d9420574",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['title'] = text_df['title'].map(lambda x: cleanText(x))\n",
    "text_df['text'] = text_df['text'].map(lambda x: cleanText(x))\n",
    "text_df['all_text'] = text_df['all_text'].map(lambda x: cleanText(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f31c71-a719-4a15-845a-4361061dc46a",
   "metadata": {},
   "source": [
    "## 5.1 Training a simple RNN model on titles only\n",
    "https://www.kaggle.com/code/therealcyberlord/fake-news-detection-using-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = news[['title','text','all_text']]\n",
    "target = news['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29e691-732b-4a59-8836-98f5a544c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize(train_data,test_data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "    train_data = tokenizer.texts_to_sequences(train_data)\n",
    "    test_data = tokenizer.texts_to_sequences(test_data)\n",
    "    vocab = tokenizer.word_index\n",
    "    return train_data,test_data,vocab\n",
    "\n",
    "def pad(train_data,test_data,max_len):\n",
    "    train_data = pad_sequences(train_data, padding='post', maxlen=max_len)\n",
    "    test_data = pad_sequences(test_data, padding='post', maxlen=max_len)\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a038fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title,X_test_title,vocab_title = tokenize(X_train['title'],X_test['title'])\n",
    "max_vocab_title = len(vocab_title) + 1 # Adding 1 because of reserved 0 index\n",
    "# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n",
    "\n",
    "# get length of longest title and pad all shorter titles to match length\n",
    "max_len_title = max([len(x) for x in X_train_title] + [len(x) for x in X_test_title])\n",
    "X_train_title,X_test_title = pad(X_train_title,X_test_title,max_len_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fda82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_vocab_title, 100, input_length=max_len_title),\n",
    "    Bidirectional(tf.keras.layers.SimpleRNN(64,  return_sequences=True)),\n",
    "    Bidirectional(tf.keras.layers.SimpleRNN(16)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47971c59-78ce-4d7f-99e9-b91691831c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_title, y_train, epochs=10, \n",
    "                    validation_split=0.15, batch_size=30, \n",
    "                    shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f87ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = history.epoch\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss', size=20)\n",
    "plt.xlabel('Epochs', size=20)\n",
    "plt.ylabel('Loss', size=20)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy', size=20)\n",
    "plt.xlabel('Epochs', size=20)\n",
    "plt.ylabel('Accuracy', size=20)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_title, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424dd77e-eb66-4348-9268-782892bcc34a",
   "metadata": {},
   "source": [
    "## 5.2 Training an LSTM model on both title and text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc9e66-e7cb-4970-9210-7c71d978c87c",
   "metadata": {},
   "source": [
    "### 5.2.1 Training the word2vec model on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219508e-3212-4c69-9e00-8a3773135ed8",
   "metadata": {},
   "source": [
    "<li>There are certain characteristics distinct to fake news that would be helpful for prediction (eg. number of capitalised words, punctuations etc. However, since word2vec trains better on words in their raw form, we will preprocess the text for this purpose while extracting the distinctive characteristics and train them in another classification model.</li>\n",
    "<li>Since the use of language and choice of vocabulary differs between real and fake news, we will not lemmatise/stem the words in our corpus</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ceee97-e8e8-4ee7-958a-d0da90e8dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only words (can have - or ') will be generated as tokens\n",
    "# create a list of elements each containing a list of words from each article in the corpus\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def corpus_to_vect(df_column):\n",
    "    corpus_tokens = []\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    rtokenizer = RegexpTokenizer(r'[a-zA-Z\\'\\-]+') # matches any word that contains only letters, hyphens, and apostrophes\n",
    "    docs = df_column.values\n",
    "    for doc in docs:\n",
    "        sents = sent_tokenize(doc)\n",
    "        doc_tokens = []\n",
    "        for sent in sents:\n",
    "            sent_lowered = sent.lower()\n",
    "            words = rtokenizer.tokenize(sent_lowered) # convert all words to lower case\n",
    "            filtered = [word.strip() for word in words if word not in stop_words and len(word) > 1] # filter out stopwords\n",
    "            doc_tokens.extend(filtered)\n",
    "        corpus_tokens.append(doc_tokens)\n",
    "    return corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3bcb2-ef3c-4284-a05a-1955f8700562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizing the 'text' column \n",
    "X_train_alltext = corpus_to_vect(X_train['all_text'])\n",
    "X_test_alltext = corpus_to_vect(X_test['all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac03e5-92a2-4569-a820-11fadf6e9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # all tokens generated in the first article\n",
    "print(X_train_alltext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b18b79-f9ce-4628-8bb8-81098f2eb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec on the 'text' corpus to form the embedding layer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "w2v = Word2Vec(\n",
    "    sentences = X_train_alltext,\n",
    "    vector_size = EMBEDDING_DIM,\n",
    "    window = 5,\n",
    "    min_count = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bdc34-51f2-45c8-b068-5e4f0c187a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(w2v.wv)\n",
    "print('There are a total of %d words in the vocabulary of our trained word2vec model.' % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5162c94-f24a-445a-98d9-d7560afcae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv[\"donald\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092b9a9-d675-4af3-a209-fc99f859c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4463ba-7eb2-4ce5-8dcb-2ef4e13d78c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2.2 Preparing the neural network model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27cfce-9421-42d1-8bd1-8ee5c1ea2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_alltext,X_test_alltext, vocab_alltext = tokenize(X_train_alltext,X_test_alltext)\n",
    "max_vocab_alltext = len(vocab_alltext) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd88625-909f-4236-8d93-9fb0f77bd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all token indexes generated from the first article\n",
    "print(X_train_alltext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e03ea-46d9-4a48-b440-27df2900e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_lens = np.array([len(index) for index in token_indices])\n",
    "plt.hist(article_lens, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadcb8b-56f5-4953-98b3-a31f94b0255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_1000 = article_lens[article_lens < 1000]\n",
    "print('%d out of %d articles have less than 1000 words' % (len(len_1000),len(article_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ffcfa-71f4-4b26-8722-2d669103ac2c",
   "metadata": {},
   "source": [
    "<li>Since the inputs to the neural network have to be of the same size, we have to pad each article in the dataset.</li>\n",
    "<li>Since the majority of articles have less than 1000 words, we pad shorter news articles and truncate longer articles.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a25628-baeb-433e-b245-de9f4946cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_alltext = 1000\n",
    "X_train_alltext,X_test_alltext = pad(X_train_alltext,X_test_alltext,max_len_alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e12edc-4237-44d6-9d1b-e904db702881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a weight matrix to retain weights learned by word2vec in the embedding layer later on\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # create weight matrix with number of rows = vocab_size and number of columns = number of embedding dimensions\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # for each word in the vocab, store its vector created by the word2vec model\n",
    "    for word, row_index in vocab.items():\n",
    "        weight_matrix[row_index] = model.wv[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134691b-9510-44d3-91b2-02095b1b749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = get_weight_matrix(w2v, vocab_alltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5fa8b0-d33e-4c83-a751-91197c0a1db0",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa62649-c773-445f-9f8a-473258600caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Dropout\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(max_vocab_alltext, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=max_len_alltext, trainable=False),\n",
    "    Bidirectional(LSTM(units=128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a9f7b-e463-4f3e-a536-52283a26ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eefc68-448f-4ef1-b00b-6e18b7efaa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d1c62-fee1-423a-84ed-9e9f9ae7b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm_model.fit(X_train_alltext, y_train, validation_split=0.2, epochs=4, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a55ed-3a02-425a-8cef-b310726eb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probs = lstm_model.predict(X_test_alltext)\n",
    "y_pred = (prediction_probs >= 0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fa809-e552-4e6a-a255-36dc42911cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4d8b8-aac4-422b-adc1-29b45d7decdf",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.drop(labels=['title','text'],axis=1,inplace=True)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a31688",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = news.loc[:,['subject','all_text', 'length']]\n",
    "target = news.loc[:,'fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news['title']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0a6d3a1c9702cbdea61a84eb923333e247e4db67d8a1f6fe21a7310eabf978f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
